{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simsam/anaconda3/envs/llm-project/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/simsam/anaconda3/envs/llm-project/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    KnowledgeGraphIndex,\n",
    "    Settings,\n",
    "    load_index_from_storage,\n",
    "    StorageContext,\n",
    ")\n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
    "\n",
    "from llama_index.graph_stores.neo4j import Neo4jGraphStore\n",
    "# from llama_index.graph_stores import NebulaGraphStore\n",
    "# from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.node_parser import HTMLNodeParser\n",
    "\n",
    "from llama_index.core.prompts.base import PromptTemplate\n",
    "from llama_index.core.prompts.prompt_type import PromptType\n",
    "\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from document_reader import HTMLDocsReader\n",
    "\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
    "\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "# Load train.json into DataFrame\n",
    "with open('data/dev.json', 'r') as f_train:\n",
    "    dev_data = pd.read_json(f_train)\n",
    "# Load documents.json into DataFrame\n",
    "with open('data/documents.json', 'r') as f_documents:\n",
    "    documents_data = pd.read_json(f_documents)\n",
    "filter_urls = dev_data['url'].unique().tolist()\n",
    "with open(\"data/documents.json\", \"r\") as file:\n",
    "    documents_data = json.load(file)\n",
    "def process_documents(documents_data, urls):\n",
    "    for document in documents_data:\n",
    "        title = document.get(\"title\", \"\")\n",
    "        url = document.get(\"url\", \"\")\n",
    "        contents = document.get(\"contents\", [])\n",
    "        if url in urls:\n",
    "            # Merge contents into a single string with newline characters\n",
    "            merged_contents = \"\\n\".join(contents)\n",
    "            # Construct text content\n",
    "            title_text =  title + '\\n'\n",
    "            contents_text =  merged_contents + '\\n'\n",
    "            # Create directory if it doesn't exist\n",
    "            if not os.path.exists(\"data/docs_dev\"):\n",
    "                os.makedirs(\"data/docs_dev\")\n",
    "            # Write content to a text file\n",
    "            file_name = f\"data/docs_dev/{title}.txt\"\n",
    "            with open(file_name, 'w') as f:\n",
    "                f.write(title_text)\n",
    "                f.write(contents_text)\n",
    "            print(f\"Document '{title}' saved successfully.\")\n",
    "process_documents(documents_data, filter_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_package = \"openai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if llm_package == 'openai':  # https://docs.llamaindex.ai/en/stable/examples/llm/openai/\n",
    "    llmmodel_name = \"\"\n",
    "    key = \"\"\n",
    "    llm = OpenAI(model=llmmodel_name, api_key=key)\n",
    "    \n",
    "    \n",
    "elif llm_package == 'mistralai':  # https://docs.llamaindex.ai/en/stable/examples/llm/mistralai/\n",
    "    api_key = \"\"\n",
    "    llm = MistralAI(api_key=api_key,model=\"open-mixtral-8x7b\")\n",
    "    \n",
    "    \n",
    "elif llm_package == 'OpenAILike':  # https://docs.llamaindex.ai/en/stable/api_reference/llms/openai_like/   # https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n",
    "    inference_server_url = \"http://0.0.0.0:8000/v1\"\n",
    "    llm = OpenAILike(\n",
    "        model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        api_key=\"token-abc123\",\n",
    "        api_base=inference_server_url,\n",
    "        max_tokens=2048,\n",
    "        temperature=0,\n",
    "    )\n",
    "    \n",
    "    \n",
    "embedding_llm = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\") # https://docs.llamaindex.ai/en/stable/examples/embeddings/huggingface/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding_llm\n",
    "\n",
    "FILEPATH = \"data/docs1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir=FILEPATH, \n",
    "    exclude=[\"*.rst\", \"*.ipynb\", \"*.py\", \"*.bat\", \"*.png\", \"*.jpg\", \"*.jpeg\", \"*.csv\", \"*.html\", \"*.js\", \"*.css\", \"*.pdf\", \"*.json\"],\n",
    "    file_extractor={\".txt\": HTMLDocsReader(tags=[\"h1\"])},\n",
    "    recursive=True\n",
    ")\n",
    "nodes = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:neo4j.notifications:Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE CONSTRAINT IF NOT EXISTS FOR (e:Entity) REQUIRE (e.id) IS UNIQUE` has no effect.} {description: `CONSTRAINT constraint_1ed05907 FOR (e:Entity) REQUIRE (e.id) IS UNIQUE` already exists.} {position: None} for query: '\\n                CREATE CONSTRAINT IF NOT EXISTS FOR (n:Entity) REQUIRE n.id IS UNIQUE;\\n                '\n",
      "Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE CONSTRAINT IF NOT EXISTS FOR (e:Entity) REQUIRE (e.id) IS UNIQUE` has no effect.} {description: `CONSTRAINT constraint_1ed05907 FOR (e:Entity) REQUIRE (e.id) IS UNIQUE` already exists.} {position: None} for query: '\\n                CREATE CONSTRAINT IF NOT EXISTS FOR (n:Entity) REQUIRE n.id IS UNIQUE;\\n                '\n"
     ]
    }
   ],
   "source": [
    "username = \"neo4j\"\n",
    "password = \"P@ssw0rd\"\n",
    "url = \"bolt://localhost:7688\"\n",
    "database = \"neo4j\"\n",
    "\n",
    "graph_store = Neo4jGraphStore(\n",
    "    username=username,\n",
    "    password=password,\n",
    "    url=url,\n",
    "    database=database\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Knowledge-Graph Triplet Extraction Prompt\n",
    "############################################\n",
    "\n",
    "DEFAULT_KG_TRIPLET_EXTRACT_TMPL_1 = (\n",
    "    \"You are a Knowledge Graph creation expert. Some text is provided below. Given the text, extract all the relevant knowledge graph triplets in the form of (subject, predicate, object). Avoid stopwords. which will contain relevant information to answer questions . \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Example:\\n\"\n",
    "    \"Text: <p>Apply by the overseas route if your acquired gender has been legally accepted in an 'approved country or territory' and you have documents to prove it.</p>\\n\"\n",
    "    \"Triplets:\\n(acquired gender, accepted in, approved country or territory)\\n\"\n",
    "    \"Text: <p>You must be 18 or over.</p>\\n\"\n",
    "    \"Triplets:\\n(you, must be, 18 or over)\\n\"\n",
    "    \"Text: <tr>Overseas route | Form T453 | Leaflet T454</tr>\\n\"\n",
    "    \"Triplets:\\n(apply, using, overseas route)\\n\"\n",
    "    \"Text: <p>If you’re applying using the overseas route, you must prove that your gender has been legally recognised in an 'approved country or territory'. Send original or certified copies of the following (if you have them):</p>\\n\"\n",
    "    \"Triplets:\\n\"\n",
    "    \"(gender, recognised in, approved country or territory)\\n\"\n",
    "    \"(send, copies of, documents)\\n\"\n",
    "    \"Text: <p>Apply by the standard route if all the following are true:</p>\\n\"\n",
    "    \"Triplets:\\n(apply, by, standard route)\\n\"\n",
    "    \"Text: <li>you’re 18 or over</li>\\n\"\n",
    "    \"Triplets:\\n(you, must be, 18 or over)\\n\"\n",
    "    \"Text: <p>You’ll get an 'interim certificate' if you or your spouse do not want to remain married, or if your spouse does not fill in a statutory declaration. You can use the interim certificate as grounds to end the marriage.</p>\\n\"\n",
    "    \"Triplets:\\n\"\n",
    "    \"(you, get, interim certificate)\\n\"\n",
    "    \"(spouse, fill in, statutory declaration)\\n\"\n",
    "    \"(use, interim certificate, end marriage)\\n\"\n",
    "    \"Text: <p>You and your spouse must fill in a statutory declaration saying you both agree to stay married.</p>\\n\"\n",
    "    \"Triplets:\\n\"\n",
    "    \"(you, spouse, fill in statutory declaration)\\n\"\n",
    "    \"(agree, stay, married)\\n\"\n",
    "    \"Text: <p>You can stay married if you apply for a Gender Recognition Certificate.</p>\\n\"\n",
    "    \"Triplets:\\n(apply for, Gender Recognition Certificate, stay married)\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Text: {text}\\n\"\n",
    "    \"Triplets:\\n\"\n",
    ")\n",
    "\n",
    "DEFAULT_KG_TRIPLET_EXTRACT_TMPL_2 = (\n",
    "    \"Some text is provided below. Given the text, extract up to \"\n",
    "    \"{max_knowledge_triplets} \"\n",
    "    \"knowledge triplets in the form of (subject, predicate, object). Avoid stopwords.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Example:\\n\"\n",
    "    \"Text: <p>Apply by the overseas route if your acquired gender has been legally accepted in an 'approved country or territory' and you have documents to prove it.</p>\\n\"\n",
    "    \"Triplets:\\n(acquired gender, accepted in, approved country or territory)\\n\"\n",
    "    \"Text: <p>You must be 18 or over.</p>\\n\"\n",
    "    \"Triplets:\\n(you, must be, 18 or over)\\n\"\n",
    "    \"Text: <tr>Overseas route | Form T453 | Leaflet T454</tr>\\n\"\n",
    "    \"Triplets:\\n(apply, using, overseas route)\\n\"\n",
    "    \"Text: <p>If you’re applying using the overseas route, you must prove that your gender has been legally recognised in an 'approved country or territory'. Send original or certified copies of the following (if you have them):</p>\\n\"\n",
    "    \"Triplets:\\n\"\n",
    "    \"(gender, recognised in, approved country or territory)\\n\"\n",
    "    \"(send, copies of, documents)\\n\"\n",
    "    \"Text: <p>Apply by the standard route if all the following are true:</p>\\n\"\n",
    "    \"Triplets:\\n(apply, by, standard route)\\n\"\n",
    "    \"Text: <li>you’re 18 or over</li>\\n\"\n",
    "    \"Triplets:\\n(you, must be, 18 or over)\\n\"\n",
    "    \"Text: <p>You’ll get an 'interim certificate' if you or your spouse do not want to remain married, or if your spouse does not fill in a statutory declaration. You can use the interim certificate as grounds to end the marriage.</p>\\n\"\n",
    "    \"Triplets:\\n\"\n",
    "    \"(you, get, interim certificate)\\n\"\n",
    "    \"(spouse, fill in, statutory declaration)\\n\"\n",
    "    \"(use, interim certificate, end marriage)\\n\"\n",
    "    \"Text: <p>You and your spouse must fill in a statutory declaration saying you both agree to stay married.</p>\\n\"\n",
    "    \"Triplets:\\n\"\n",
    "    \"(you, spouse, fill in statutory declaration)\\n\"\n",
    "    \"(agree, stay, married)\\n\"\n",
    "    \"Text: <p>You can stay married if you apply for a Gender Recognition Certificate.</p>\\n\"\n",
    "    \"Triplets:\\n(apply for, Gender Recognition Certificate, stay married)\\n\"\n",
    "    \"Text: {text}\\n\"\n",
    "    \"Triplets:\\n\"\n",
    ")\n",
    "\n",
    "DEFAULT_KG_TRIPLET_EXTRACT_TMPL = (\n",
    "    \"Some text is provided below. Given the text, extract up to \"\n",
    "    \"{max_knowledge_triplets} \"\n",
    "    \"knowledge triplets in the form of (subject, predicate, object). Avoid stopwords.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Example:\"\n",
    "    \"Text: Alice is Bob's mother.\"\n",
    "    \"Triplets:\\n(Alice, is mother of, Bob)\\n\"\n",
    "    \"Text: Philz is a coffee shop founded in Berkeley in 1982.\\n\"\n",
    "    \"Triplets:\\n\"\n",
    "    \"(Philz, is, coffee shop)\\n\"\n",
    "    \"(Philz, founded in, Berkeley)\\n\"\n",
    "    \"(Philz, founded in, 1982)\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Text: {text}\\n\"\n",
    "    \"Triplets:\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KG_TRIPLET_EXTRACT_PROMPT = PromptTemplate(\n",
    "    DEFAULT_KG_TRIPLET_EXTRACT_TMPL,    # prompt template\n",
    "    prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT,\n",
    ")\n",
    "\n",
    "max_triplets_per_chunk = 25     # maximum number of triplets to extract per chunk\n",
    "\n",
    "PERSIST_DIR = \"./storage/storage_graph_kg_prompt_1\"   # directory to store the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes:   7%|▋         | 1/15 [00:05<01:18,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes:  13%|█▎        | 2/15 [00:07<00:47,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes:  20%|██        | 3/15 [00:10<00:35,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes:  27%|██▋       | 4/15 [00:15<00:42,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes:  33%|███▎      | 5/15 [00:17<00:32,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes:  40%|████      | 6/15 [00:18<00:23,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes:  47%|████▋     | 7/15 [00:19<00:16,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes:  53%|█████▎    | 8/15 [00:25<00:21,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes:  60%|██████    | 9/15 [00:27<00:17,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes:  67%|██████▋   | 10/15 [00:29<00:13,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes:  73%|███████▎  | 11/15 [00:30<00:08,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes:  80%|████████  | 12/15 [00:32<00:06,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes:  87%|████████▋ | 13/15 [00:35<00:04,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes:  93%|█████████▎| 14/15 [00:36<00:01,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes: 100%|██████████| 15/15 [00:38<00:00,  2.55s/it]\n"
     ]
    }
   ],
   "source": [
    "graph_storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "\n",
    "\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    kg_index  = KnowledgeGraphIndex(nodes,\n",
    "                    storage_context=graph_storage_context, \n",
    "                    max_triplets_per_chunk=max_triplets_per_chunk,\n",
    "                    show_progress=True,\n",
    "                    kg_triple_extract_template=KG_TRIPLET_EXTRACT_PROMPT,\n",
    "                    include_embeddings = True\n",
    "                )\n",
    "    kg_index.set_index_id(\"kg_index\")\n",
    "    kg_index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    graph_storage_context = StorageContext.from_defaults(graph_store=graph_store,persist_dir=PERSIST_DIR)\n",
    "    kg_index = load_index_from_storage(graph_storage_context, index_id=\"kg_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'COUNT(*)': 125}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_store.query(\n",
    "    \"\"\"\n",
    "MATCH (m)-[e]->(n) RETURN COUNT(*)\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# graph_store.query(\n",
    "#     \"\"\"\n",
    "# MATCH (n)\n",
    "# DETACH DELETE n\n",
    "# \"\"\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
